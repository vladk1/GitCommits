\documentclass{sig-alternate}
\usepackage{blindtext, graphicx}
\usepackage{fancyhdr} 
\usepackage{txfonts}
\usepackage{lipsum}
\usepackage{booktabs}
\usepackage{gitinfo}

\graphicspath{ {images/} }

\newcommand\FIXME[1]{\textbf{FIXME: #1}}

\begin{document}
\conferenceinfo{MSR}{'16 Austin, Texas USA}
\title{Investigating Fault-Proneness Through Hotfix Commits in the Light of Different Programming Languages}
%\numberofauthors{6}
\author{
  Martin Lazarov, Ismaa'eel Ataullah, Vladyslav Kolesnyk, Veselin Pavlov,\\ Chaiyong Ragkhitwetsagul, Jens Krinke\\
  \affaddr{University College London}\\
  \affaddr{London, UK}
}

\maketitle

\begin{abstract}
Choosing the most suitable programming language is a challenging task for every project manager and developer. Often the success of a software project is determined by the stability of the system after a release. We analyse the prevalence of hotfix commits in various development areas as an indicator of which languages tend to produce the least amount of problems after software has been published. Based on the Boa September 2013 SourceForge dataset, we analysed the metadata from the 16 most popular project topics and estimated the amount of deployed hotfixes to compare the fault-proneness of programming languages that are used across project domains. Our results only identified 8 languages that are widely used across different domains: Java, C, C++, C\#, Perl, Python, PHP, and JavaScript. Java has the least number of hotfixes per version update (0.18), while C\# required most hotfixes per version update (0.34). We identified the Security and Internet \& Communications project topic as being the most reliable (i.e.\ requiring least amount of hotfixes per version update). The analysis did not reveal conclusive patterns for language classes (strongly vs.\ weakly typed and dynamic vs.\ static).
\end{abstract}

% A category with the (minimum) three required fields
%\category{D.2.7}{Software Engineering}{\FIXME{}}
%A category including the fourth, optional field follows...

%\terms{Experimentation, Measurement, Languages, Performance}

%\keywords{Version Control, hotfixes, SourceForge, commits metadata}

\section{Introduction}
Planning and resource allocation are arguably the most crucial components of any software project. In the planning phase, project managers often neglect the importance of hotfixes -- which becomes essential once a project is deployed.  A hotfix is correcting a critical problem which prevents a particular software product from performing as expected. This could mean a total or partial loss of service, a major security breach that has to be patched instantly, etc. As a result, such a fix is done outside of normal development and testing procedures. The characteristic of hotfixes implies that rapid development under urgent circumstances is usually required. Therefore, it can be often the case that additional resources need to be allocated quickly for the correction, which leads to a significant increase in the cost of the overall product.

% Another aspect that makes hotfixes so important is their unpredictability which further prevents teams from planning in advance for such corrections into their development cycle. Moreover, hotfixes are sometimes delayed on purpose because people are afraid of the risk of applying the fix. All of these factors contribute to increasing the uncertainty and stress in managers and that can prevent them from taking the most appropriate action when a problem in production has to be resolved promptly.

There are no clear definitions of what a hotfix is, we define a hotfix as \emph{a corrective commit deployed shortly after a project's release to fix a critical issue}. Inspired by work on the impact of programming languages on code quality, we investigated how programming languages affect the frequency of hotfixes. Because project domains strongly influence the choice of a programming language, we restricted our analysis to programming languages that are used \emph{across} project domains.

%With our research we aim to help project managers and developers make a more informed decision when deciding what languages to use by comparing the number of hotfixes per programming language. To provide a better picture we also look at hotfixes per specific software topics. 

%We believe that the results from our research can provide insights into the average number of issues per project with a focus on different programming topics and the relevant languages used. Consequently, project managers will be able to make a more informed decision with regard to the most appropriate programming languages they could use to try to achieve a minimum number of problems in production. This will improve software engineering practices and the overall project planning activity. Furthermore, it will help optimise the allocation of resources.

The main contributions of this paper are:
%
\begin{itemize}
  \item An  approach to identify hotfix commits in repositories.
  \item A quantitative analysis identifying fault-proneness of specific programming topics and the distribution of faults across programming languages. The analysis identified that only eight languages are used across the top 16 most popular domains (topics) identified by the Boa SourceForge 2013 dataset.
  \item An investigation on the impact of language classes on reliability: strongly-typed vs weakly-typed, static vs dynamic and scripting vs procedural. The investigation showed no clear patterns on the influence of language classes.
\end{itemize}
%
Related work is discussed in the next section. Section 3 explains our empirical study and its results, followed by conclusions and future work.

\section{Related Work}
%Many researchers have explored various aspects of defects in software through the use of versioning system history logs. We have also decided to follow this approach due to its openness and accessibility. This method is also very widely adopted amongst the MSR community \cite{Hattori2008}. 

Our investigation focused primarily on two research aspects and existing work related to 1) comparison of programming languages impact, and 2) identifying hotfix commits.

% Another solution related to identifying defects in software focused on analysing commit metadata to discover design smells and predict where a break in different areas of the code may appear every time a change is performed \cite{Oliva2013}. However, they did not make their investigation language-specific as opposed to what we propose in our paper.\par

% Other scientists tried to explore the logical coupling of commits as a way of identifying potential software defects. They aimed at developing a method to reduce the number of bugs in the code during the development stage so that less fix commits will be needed in the future \cite{Steff2012}.\par

% A further interesting research that we found was related to the use of separate bug fix branches. According to the study performed by Phillips et al. \cite{Phillips2011}, 41\% of the tested projects had incorporated an isolated bug fix branch.


\subsection{Comparison of Programming Languages}
%Throughout our investigation we discovered that there were a lot of authors that tried to utilize commit data to try to propose good engineering practices and find a way to determine code quality \cite{Agrawal2015}. Furthermore, there were several papers that compared and contrasted the performance of different languages, which led to our second research topic. We explored these existing scientific studies to see what techniques authors had implemented. We also analysed the hypotheses these papers proposed to increase the scope of our project.\par

%Our specific point of interest was related to different software topics and programming languages to discover how number of hotfixes per topic depend on the language developers use.\par

The main related work focussing on the effect of programming languages~\cite{Ray2014} is based on the rate of defect occurrences in various languages. The study was carried out on a huge dataset from GitHub (1.5 million commits). To evaluate the effect that programming languages have on software quality, the authors grouped languages according to their class, e.g.\ strongly vs.\ weakly typed. They also used mathematical methods to triangulate and produce statistics for how group size, project history and number of contributors affect the number of defects. In order to achieve more representative results, they took a sample of 17 languages and extracted the top 50 projects per language. The results showed how defect-prone languages are compared to others. This research focused on classifying bugs based on their type, while we focused specifically on the distribution of hotfixes across programming languages.

Phipps et al.~\cite{Phipps1999} evaluated Java and C++ by comparing bug rates. There were two definitions for bugs: defect and bug. Defects included issues such as syntax errors, logical errors and changes in variable name. A bug itself was only defined to be a problem detected during testing or deployment. The mean number of defects and bugs, averaged over a number of projects, was compared between both languages per 1000 lines of code. The work also observed defects and bugs per hour as well as the time taken to fix defects in both languages. Productivity was measured by analysing number of lines written per second.

Other work used different parameters to compare programming languages, for example, program length, programming effort, run-time efficiency, memory consumption, and reliability~\cite{Prechelt2000}.  Fourment and Gilling~\cite{Fourment2008} analysed memory usage and speed of execution of three different bioinformatics methods for six distinct programming languages. This was especially pertinent as bioinformatics applications have huge datasets and computational time taken is not trivial, hence optimising speed is desirable.

Another recent work~\cite{Nanz2015} compared 8 different languages by conciseness, size of executables, running time, memory usage and failure proneness, arguing that conciseness is an important factor as a more compact language can help write a program with fewer bugs.

\subsection{Identifying Bug Fix Commits}
%The first major problem we had to solve was identifying actual hotfix commits. Due to Boa's limitations, we were not able to find commit history from hotfix branch. We were only capable of collecting information with regard to commits performed on the main branch. Therefore, we reviewed various papers to identify the most appropriate method of determining the type of the commit to extract only the ones related to a bug correction.\par

A large number of previous work uses a simple approach to identify bug-fixing commits. The commit message was employed as the main mechanism to identify whether a commit was of a corrective type by simply investigating if the message contained particular key words, such as ``bug'', ``fix'', ``issue'', ``error'', ``broke'', etc.~\cite{Hattori2008, Ratzinger2008, Hindle2008}.  Other authors proposed a solution that involved a slightly more complex procedure including a syntactic analysis of the commit messages. Their algorithm consisted of multiple steps: 1) normalization of the data (removing non-alphanumeric characters), 2) word frequency analysis including a classification of the most common words ( e.g. ``for'', ``code'', ``to'', ``the'', etc. ) as being neutral, 3) implementing a keyword clustering algorithm to classify the commits~\cite{Mockus2000}.

Another approach to identify bug fix commits performs a classification based on the size of the commits~\cite{Hattori2008,Mockus2000,Hindle2008}. The size of a commit can be measured based on the number of lines or the amount of files that were changed. Large commits tend to improve code quality or add new features, while small ones are considered to be corrective -- targeted at fixing bugs in the project. A fine-grained size classification depending on the number of files changed per commit defined the following categories: 1) tiny (1 to 5 changed files), 2) small (6 to 25 files), 3) medium (26 to 125 files), 4) large (> 125 files) ~\cite{Hattori2008}. The authors argued that most of the commits tend to be tiny (in 80\% of the cases) which implies that bug fix commits occur very often as corrective commits tend to be small ones.

Issues in critical components (e.g.~errors in operating system drivers, security errors) tend to be fixed relatively quickly when discovered~\cite{Livshits2005}. Mockus and Votta~\cite{Mockus2000} argue that the time between changes will depend on the type of alteration and that corrective changes require the least amount of time to be completed as they often have to be performed promptly regardless of the complexity of the task.

Williams and Spacco~\cite{Williams2008} investigated the duration between the moment when a change is applied to a specific branch and the moment the new code was merged into the trunk. However, they explored branches in general without focusing specifically on bugfix branches. Kolassa et al.~\cite{Kolassa2013} try to explain people's behaviour during commits by investigating their commit frequency distribution. They discovered that on average the time between commits was 3.206 days and half of all the contributors spent less than 13.78 hours between 50\% of their commits.



\section{Empirical Study}

% \begin{table}
% \centering
% \resizebox{\columnwidth}{!}{%
% \begin{tabular}{r|lll}
% \multicolumn{1}{r}{}
% & \multicolumn{1}{l}{Heading 1}
% & \multicolumn{1}{l}{Heading 2}
% & \multicolumn{1}{l}{Heading 3} \\ \cline{2-4}
% Row 1 & Cell 1,1 & Cell 1,2 & Cell 1,3 \\
% Row 2 & Cell 2,1 & Cell 2,2 & Cell 2,3
% \end{tabular}%
% }
% \end{table}

Our work was inspired by a previous research paper~\cite{Ray2014} which compared the number of bug fix commits to the total number of commits across 17 languages and 850 different projects. Our primary contribution is the investigation of the fault-proneness of different languages \emph{across} project domains (topics) based on the newly defined criteria \emph{hotfixes per version update}. 

\subsection{Dataset}
We selected the Boa September 2013 SourceFourge dataset for this study as it contains project topics. From the dataset, we selected projects from the top 16 most popular project topics. This resulted in of 49,865 projects. Moreover, we only concentrate on projects that incorporate versioning (i.e.\ containing bump commits).

\subsection{Approach for Identifying Hotfix Commits}
Our method to distinguish hotfixes based on commit messages has two parts:

\textbf{1) Detecting version bump commits:} From the observed data, it turned out to be straightforward to extract this information by a regular expression that matches the following strings: ``bump'', ``version'', ``update'' in an appropriate order.

\textbf{2) Detecting hotfix commits:} Due to the similarity between commit messages for normal bug fixes and hotfix commits, it was challenging to distinguish them only based on the message. Therefore, we applied two additional filters: time and range of commits.

\textbf{Time} -- hotfix commits have to be submitted within 1 day from the version bump commit. Thus, a hotfix is assigned the highest priority and has to be submitted rapidly. Furthermore, it is very likely that fixes performed on the first day after a new release will refer to problems occurring before the version increase.

\textbf{Look-ahead range for commits} -- only n numbers of commits should be considered after a version bump. We focused on the Look-ahead parameter and decided to investigate only the first 10 commits as any commit after this point would possibly be related to code added after the release rather than before that. We have performed an initial investigation of 5, 10 and 15 commits and found that 10 achieved the best balance between high number of fix commits and low number of commits that introduce new features.

\subsection{Research Questions}
\noindent Our study aimed to answer the following research questions:

\noindent\textbf{RQ1 (language popularity):} Which languages are the most widely used across the top 16 most popular topics?

\noindent\textbf{RQ2 (hotfixes vs.\ project topic):} Which project topics achieve the highest/lowest number of hotfixes per version update?

\noindent\textbf{RQ3 (hotfixes vs.\ languages):} Which languages achieve the highest/lowest number of hotfixes per version update?

\noindent\textbf{RQ4 (hotfixes vs.\ language class):} Are there any relationships between the occurrences of hotfixes and the class of the programming language?

\begin{table*}[!htbp]
	\centering
	\small
	\begin{tabular}{@{}l c@{~~}c@{~~}c r r r r r r r r r r r r r r r r r@{}}
		Language & $\mathcal{P}$ & $\mathcal{C}$ & $\mathcal{T}$ & 
		\rotatebox{90}{Databases} & \rotatebox{90}{Systems Administration} & 
		\rotatebox{90}{Dynamic Content} & 
		\rotatebox{90}{Front Ends} & 
		\rotatebox{90}{Site Management} & 
		\rotatebox{90}{Role Playing} & \rotatebox{90}{Games} & \rotatebox{90}{Security} &
		\rotatebox{90}{Internet} & \rotatebox{90}{Software Development} & 
		\rotatebox{90}{Testing} & 
		\rotatebox{90}{Build Tools} & 
		\rotatebox{90}{Code Generators}&
		\rotatebox{90}{Communications} & \rotatebox{90}{Education} & \rotatebox{90}{Frameworks} &
		\rotatebox{90}{Average} \\
		\hline
		Java & P & S & S & 0.25 & 0.15 & 0.22 & 0.08 & 0.28 & 0.32 & 0.07 & 0.22 & 0.38 & 0.17 & 0.23 & 0.10 & 0.06 & 0.09 & 0.10 & 0.09 & \textbf{0.176} \\
		C\#  & P & S & S & -&0.20&-&-&0.30&0.89&0.14&0.27&0.60& 0.08 & - & 0.07 & - & 0.57 & - & 0.27 & \textbf{0.339} \\
	    C++  & P & S & W & 0.24&0.12&0.26&0.06&-&0.51&0.35&0.31&0.20&0.26&0.27&0.35&0.73&0.08&0.12&0.43 & \textbf{0.286} \\ 
		C  & P & S & W & 0.29&0.24&0.29&0.33&0.43&0.56&0.39&0.05&0.03&0.34&0.27&0.33&0.73&0.07&0.23& - & \textbf{0.305}\\
		PHP  & S & D & W & 0.27&0.23&0.27&0.28&0.24&-&-&0.23&0.10&0.38&-&-&0.14&-&0.67&0.31&\textbf{0.282} \\		
		Perl  & S & D & S & 0.17 & 0.05 & 0.22 & 0.44 & - & 0.41 & - & 0.06 & 0.13&0.24&-&-&-&0.17&0.19&0.32 & \textbf{0.218} \\
		Python	& S & D & S & 0.27	& 0.24	& 0.36	& -	& - & 0.40 & 0.42	& 0.13	& -	& 0.41	& -	& 0.33	& -	& 0.14	& 0.22	& 0.36	& \textbf{0.298} \\
		JavaScript  & S & D & W & 0.26 & - & 0.27 & - & 0.17 & - & 0.17 & 0.23 & - & - & - & - & - & - & 0.18 & 0.18 & \textbf{0.209} \\
	
		\hline
		Average & &  & & \textbf{0.25}&\textbf{0.17}&\textbf{0.28}&\textbf{0.28}&\textbf{0.28}&\textbf{0.47}&\textbf{0.33}&\textbf{0.17}&\textbf{0.12}&\textbf{0.32}&\textbf{0.27}&\textbf{0.34}&\textbf{0.53}&\textbf{0.11}&\textbf{0.27}&\textbf{0.32} \\
		\hline
	\end{tabular}
	\caption{Ratios of hotfix commits per version bump commits of 8 common languages and their 16 respective topics. $\mathcal{P}$, $\mathcal{C}$, and $\mathcal{T}$ represent the classes of the language. $\mathcal{P}$ stands for Programming Paradigm class (P=Procedural, S=Scripting). $\mathcal{C}$ is for the Compilation class (S=Static, D=Dynamic), and $\mathcal{T}$ is for the Type class (W=weakly-typed, S=Strongly-typed).}
	\label{tab:1}
\end{table*}

%\section{Results}
We performed an empirical study on the SourceForge dataset using our method of identifying version bump commits and hotfix commits. The findings are explained below.
\subsection{RQ1: Language Popularity}
Our investigation on the most widely used languages across the top 16 most popular topics revealed that there are surprisingly few such languages. We chose the languages based on their frequency of occurrence across 16 topics. From Table \ref{tab:1}, the 8 widely used languages include Java (16), C (15), C++ (15), Perl (11), Python (11), PHP (11), C\# (10), and JavaScript (7) respectively.  We found a significant drop in occurrence for the rest of languages so their results are not included here.  %One would expect JavaScript to appear in the list. However, it is present only in 7 categories and hence is ignored from the list. This may be due to the fact that JavaScript is mainly used only for website development while our 16 popular categories span over various aspects of software. 

It is interesting that the majority of these languages are not used in Testing, which contains only Java, C, and C++. Similarity, only a few languages appear in the Code Generators and the Site Management topics.

% From Table \ref{tab:1}, \textbf{the most widely used languages from the list is Java which appears in all the 16 categories.} C, and C++ share the second place with occurrence in 15 out of 16 categories. While C++ is not present in Site Management, C is not present in Frameworks. The rest of the languages in the list are present in 10 categories on average. However, it is interesting to note that the majority of these languages are not used in Testing, which contains only Java, C, and C++. Similarity, only a few languages appears in Code Generators and Site Management categories.

\subsection{RQ2: Hotfixes vs.\ Project Topics}
To be able to compare among languages, we calculated their hotfix ratios. The hotfix ratio of a language $P$ in topic $C$ is a ratio of total hotfix commits per total version bump commits of the language in that topic. %We proceed to analyse why certain categories might have low numbers of hotfixes and vice versa.
\begin{equation*}
\mathrm{HotfixRatio}_{P,C} = \frac{\sum{\mathrm{HotfixCommit}_{P,C}}}{\sum{\mathrm{VersionBumpCommit}_{P,C}}}
\end{equation*}
In Table \ref{tab:1}, \textbf{Communications have the lowest average hotfix ratios across all topics (0.11)} followed by Internet, Security, and Systems Administration (0.12, 0.17, 0.17) respectively. These topics are information critical. They are expected to be tested extensively before deployment to reduce the chance of failure. This may result in fewer hotfixes being needed after deployment. An interesting observation is that the average hotfix ratio of C in the Internet topic is incredibly low (0.03). We believe that is because there are several new languages designed specifically for Internet applications. Not much new C code is being written in this area, which results in a very low number of hotfix commits. %Security categories  %This could be down to fully comprehensive testing techniques, but could also be attributed to the fact that within those categories, the language has been used for an extensive period and hence there is a lot of experience with a variety of different bugs, meaning they are less likely to occur during development as experienced programmers are less likely to make those particular mistakes in the first place. This could also be the reason the projects written in Perl have a very low hot fix average for all four categories.

On the other hand, the topic with \textbf{the highest average hotfix ratio is Code Generators (0.53)}. In this topic, C and C++ have very high average hot fix ratios while  PHP and Java have the lowest. One can speculate that PHP and Java are established languages for writing code generators and due to this engender low hot fix averages. C and C++ might share syntactic details and semantic structure that would result in a similar hot fix average between them. 

The second least reliable topic is Role playing (0.47) which has higher hotfix ratio than its superset, the Game topic. We believe that this specific kind of game is more complex with vast amount of configurations of characters and items in the game. It is common to release a few hotfixes after a new version release due to adjusting these configurations. %We can obviously see that all languages in Role Playing category have high average numbers of hot fixes per update.

%Applications under the categories as Security, Internet, Communications and Systems administration are all information critical, which means they have critical assets that need to be protected in order to maintain customer's privacy and trust. For example, with Communications applications it is vital that security is implemented in such a way that an adversary cannot intercept or obtain messages only intended for the receiver. This means that applications that fall under such categories should be extensively tested before deployment to reduce the chance of failure. This results in less hot fix being needed after deployment.

%Role Playing applications have a high average number of hot fixes compared to other projects that fall under the Games category. It is possible that the Games category is a general category. It is a lot simpler than those that fall under the role playing category which is why bugs might be more prominent as a result of increased complexity of the game. Moreover, usually this kind of game contains huge amount of configurations of characters, and items in the game. It is common to release a few hotfixes after releasing a new version due to fixing these configurations. We can obviously see that all languages in Role Playing category have high average numbers of hot fixes per update.

%On the other hand, for Code generators, C and C++, have very high hot fix averages while  PHP and Java are the lowest. One can speculate that PHP and Java are established languages for writing code generators and due to this engender low hot fix averages. C and C++ might share syntactic details and semantic structure that would result in a similar hot fix average between them.

\subsection{RQ3: Hotfixes vs.\ Languages}
The language comparison based on the average hotfix ratio over all the topics in Table 1 has shown that \textbf{Java has the lowest average hotfix ratio (0.176)} followed by JavaScript (0.209) and Perl (0.218). However, all the mentioned languages show marginally better results compared to other common languages: PHP (0.282), C++ (0.286), C (0.305) and C\#(0.339). \textbf{C\# has the highest average hotfix ratio among all the languages.}

\subsection{RQ4: Hotfixes vs.\ Language Classes}
Another interesting research question is comparing the class of programming languages by their average hotfix ratios. The classification in Table \ref{tab:1} is inspired by Ray et al.~\cite{Ray2014}, where languages were compared using hotfix ratios with respect to their three main classes. The Programming Paradigm class (noted as $\mathcal{P}$ in Table \ref{tab:1}) has three variations: procedural, functional, and scripting. The Compilation class contains static and dynamic typing. The final Type class separates languages into strongly-typed and weakly-typed. According to Ray et al.~\cite{Ray2014}, strongly-typed languages experienced fewer bug fixes than its counterpart, weakly-typed. Static programming languages required fewer corrective commits than dynamic ones. And finally, functional languages encountered fewer bug fixes than procedural ones.

Due to our criteria of selecting the languages, the chosen 8 languages are all either procedural or scripting, but not functional. Most probably this was due to the fact that functional languages are limited to specific use cases and as such are not widely popular across different domains~\cite{Wadler1999}. Consequently, we only compared procedural and scripting languages in this class.

\subsubsection*{Programming Paradigm: Procedural vs.\ Scripting}
From Table \ref{tab:1}, there is no clear trend or pattern of the hotfix ratio among procedural (Java, C, C\#, C++) and scripting (Perl, Python, PHP) languages across all the topics. This is possibly because the number of data points (languages) were too low to observe such behaviours and draw any firm conclusions. %It was always the case that at least one of the languages would not follow a pattern. For some categories, the comparison would consider only one language from a certain category.

\subsubsection*{Compilation: Static vs.\ Dynamic}
In this class, static (Java, C, C\#, C++) and dynamic (Perl, Python, PHP) languages led to the same conclusions of no clear difference of observed hotfix ratios between the two sets as can be seen from Table \ref{tab:1}. The reason was because the languages in these two classes coincided with the languages in the first two classes (procedural and scripting).

\subsubsection*{Type: Strongly-Typed vs.\ Weakly-Typed}
The third class compares strongly-typed (Java, C\#, Perl, Python) and weakly-typed (C, C++, PHP) languages. Similarly to the previous two classes, we encountered no clear pattern across the topics. Furthermore, we found problems of having comparisons against only one member of a language class. Solely in the Communications topic, we were able to detect some pattern, where weakly-typed languages performed slightly better. However, there were only two languages labelled as weak. Therefore, we cannot reach any satisfying conclusions based on this finding.

Overall, \textbf{we could not identify any convincing patterns based on the class comparison between languages} mostly due to the low number of selected programming languages.

\subsection{Limitations}
There were several limitations that we encountered throughout our study. First, we were able to analyse only a small part of all available projects because most of the developers did not support a versioning system, i.e. no bump commits could be identified.

%Therefore, we may have introduced some false positives or failed to account for actual hotfixes.
Moreover, we could not extract hotfix branches, only the overall commits on the trunk were available and thus we used our own parameters to try to distinguish hotfix commits. The way we detected bump commits was by writing a regular expression to match words like:  ``bump'', ``version'', ``update'' in an appropriate order. Thus, we may generate false positives by including version updates that are not real hotfixes, and also false negatives by excluding version updates had they been indicated differently.

Most open source projects are not under the stress of releasing as opposed to commercial projects. Consequently, they do not release very often and/or do not support versioning.

There was a low amount of languages that satisfied our predicate (being present in at least one half of all evaluated topics), which did not allow us to state any conclusive results. In addition, with a limited number of programming topics used (16 in total), we are restricted to observe distinguishable patterns.

Lastly, only one dataset was used to evaluate our hypothesis: SourceForge September 2013. Hence, these results may not be generalised.

\section{Conclusions and Future Work}
We have presented a study on detecting the fault-proneness of programming languages through hotfix commits, a topic that has not been investigated so far. As part of our research we performed a topic-specific analysis across 16 programming domains (topics) to detect the most widely used languages. Languages and topics that achieved highest/lowest ratios of hotfix per version update were investigated and we tried to identify patterns based on the language classification presented by Ray et al.~\cite{Ray2014}. We used Boa to mine repositories in the SourceForge September 2013 dataset. %Our goal was to verify whether the general relationships between languages' reliability holds with regard to hotfixes.  
We identified 8 languages as being the most popular across different domains including: Java, C, C++, Perl, Python, PHP, C\#, and JavaScript. We discovered that Java achieves the smallest hotfix/version update ratio (0.176), while C\# required the highest amount of hotfixes per version increase (0.339). From the different topics, we observed that Security, Internet and Communications are the most reliable (i.e. achieving the lowest hotfix/version update ratios), while Role Playing and Code Generation experienced the highest number of corrections by hotfixes. Finally, according to the language classes, we were not able to identify any conclusive patterns.

We believe that there are some aspects to be improved in our study. In particular, more datasets could be used, including Github (provided that projects are categorised into topics), and more reliable method to identify bump commits. Instead of using data from SourceForge, real commercial projects which have been open-sourced can be mined to enhance the quality of our data. In such projects, releases should occur more frequently and thus, there is a higher probability of identifying more hotfixes.

Finally, we hope that our paper lays a good foundation for future research in this area. We expect that the results we obtained will help developers and project managers improve their planning and use the most appropriate languages for a specific topic or purpose.

\bibliographystyle{abbrv}
\bibliography{hotfixes} 

% that's all folks
\end{document}