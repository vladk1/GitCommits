% \documentclass[conference]{IEEEtran}

% \begin{document}
% \title{An Important Conference Contribution}

% \author{\IEEEauthorblockN{Author One\IEEEauthorrefmark{1},
% Author Two\IEEEauthorrefmark{2}, Author Three\IEEEauthorrefmark{3} and
% Author Four\IEEEauthorrefmark{4}}
% \IEEEauthorblockA{Department of Whatever,
% Whichever University\\
% Wherever\\
% Email: \IEEEauthorrefmark{1}author.one@add.on.net,
% \IEEEauthorrefmark{2}author.two@add.on.net,
% \IEEEauthorrefmark{3}author.three@add.on.net,
% \IEEEauthorrefmark{4}author.four@add.on.net}}
% \maketitle

% \end{document}

\documentclass{sig-alternate}
\usepackage{blindtext, graphicx}
\usepackage{fancyhdr} 
\usepackage{txfonts}
\usepackage{lipsum}
\usepackage{booktabs}
\usepackage{gitinfo}

\graphicspath{ {images/} }

\newcommand\FIXME[1]{\textbf{FIXME: #1}}
%\fancyhf{}
%\cfoot{\thepage}
%\pagestyle{fancy}

% *** GRAPHICS RELATED PACKAGES ***
%
%\ifCLASSINFOpdf
  % \usepackage[pdftex]{graphicx}
  % declare the path(s) where your graphic files are
  % \graphicspath{{../pdf/}{../jpeg/}}
  % and their extensions so you won't have to specify these with
  % every instance of \includegraphics
  % \DeclareGraphicsExtensions{.pdf,.jpeg,.png}
%\else
  % or other class option (dvipsone, dvipdf, if not using dvips). graphicx
  % will default to the driver specified in the system graphics.cfg if no
  % driver is specified.
  % \usepackage[dvips]{graphicx}
  % declare the path(s) where your graphic files are
  % \graphicspath{{../eps/}}
  % and their extensions so you won't have to specify these with
  % every instance of \includegraphics
  % \DeclareGraphicsExtensions{.eps}
%\fi

% correct bad hyphenation here
%\hyphenation{op-tical net-works semi-conduc-tor}

%\pagenumbering{arabic}
\begin{document}
\conferenceinfo{MSR}{'16 Austin, Texas USA}
%
% paper title
% can use linebreaks \\ within to get better formatting as desired

\title{Investigating Fault-Proneness Through Hotfix Commits:\\A Research in the Light of Different Programming Languages}
\numberofauthors{4}
\author{
  Martin Lazarov, Ismaa'eel Ataullah, Vladyslav Kolesnyk, Veselin Pavlov\\
  \affaddr{University College London}\\
  \affaddr{London, UK}
}


%\title{Investigating Fault-Proneness Through Hotfix Commits: A Research in the Light of Different Programming Languages}

% author names and affiliations
% use a multiple column layout for up to three different
% affiliations
%\author{\IEEEauthorblockN{Martin Lazarov\IEEEauthorrefmark{1},
%Ismaa'eel Ataullah\IEEEauthorrefmark{2}, Vladyslav Kolesnyk\IEEEauthorrefmark{3},
%Veselin Pavlov\IEEEauthorrefmark{4}}
%\IEEEauthorblockA{Department of Computer Science,
%UCL\\
%Email: \IEEEauthorrefmark{1}martin.lazarov.12@ucl.ac.uk,
%\IEEEauthorrefmark{2}ismaaeel.ataullah.12@ucl.ac.uk,
%\IEEEauthorrefmark{3}vladyslav.kolesnyk.12@ucl.ac.uk,
%\IEEEauthorrefmark{4}veselin.pavlov.15@ucl.ac.uk}}

% make the title area
\maketitle

\begin{abstract}
%\boldmath
Choosing the most suitable programming language is a very challenging task for every project manager and developer. Often the success of a software project is determined by the stability of the system after a release. In this paper, we propose analysing hotfix commits in various development areas as a means of indicating what languages tend to produce the least amount of problems after software has been published. We have focused on parsing metadata from the 16 most popular projects hosted on SourceForge and have performed an estimation of the amount of deployed hotfixes to compare the fault-proneness of various languages. 
\end{abstract}

% A category with the (minimum) three required fields
\category{D.2.7}{Software Engineering}{\FIXME{}}
%A category including the fourth, optional field follows...

%\terms{Experimentation, Measurement, Languages, Performance}

\keywords{Version Control, hotfixes, SourceForge, commits metadata}

%\begin{IEEEkeywords}
%Version Control, hotfixes, SourceForge, commits metadata.
%\end{IEEEkeywords}

\section{Introduction}
Planning and resource allocation are arguably the most crucial components of any software project. Our research was motivated by the fact that very often, in the planning phase project managers neglect the importance of hotfixes - which becomes essential once a project is deployed.

Generally, a hotfix is related to correcting a critical problem which prevents a particular software product from performing as expected. This could mean a total or partial loss of service, a major security breach that has to be patched instantly, etc. As a result, such a fix is done outside of normal development and testing procedures. This characteristic of hotfixes implies that rapid development under urgent circumstances is usually required. Therefore, it can be often the case that lots of additional people and resources need to be allocated quickly for the correction, which leads to a significant increase in the cost of the overall product.

Another aspect that makes hotfixes so important is their unpredictability which further prevents teams from planning in advance for such corrections into their development cycle. Moreover, hotfixes are sometimes delayed on purpose because people are afraid of the risk of applying the fix. All of these factors contribute to increasing the uncertainty and stress in managers and that can prevent them from taking the most appropriate action when a problem in production has to be resolved promptly.

As there are no clear definitions that specify what a hotfix is, according to our research and observations from the open-source projects, a hotfix can be defined as: ``a corrective commit deployed within 1 day, following a project's version update to fix critical issues in the release''.\par

With our research we aim to help project managers and developers make a more informed decision when deciding what languages to use by comparing the number of hotfixes per programming language. To provide a better picture we also look at hotfixes per specific software topics. 

We believe that the results from our research can provide insights into the average number of issues per project with a focus on different programming topics and the relevant languages used. Consequently, project managers will be able to make a more informed decision with regard to the most appropriate programming languages they could use to try to achieve a minimum number of problems in production. This will improve software engineering practices and the overall project planning activity. Furthermore, it will help optimise the allocation of resources.
The contributions of this paper are:

\begin{itemize}
  \item parameters to determine most probable hotfix commits in the repository.
  \item a quantitative analysis identifying fault-proneness of specific programming topics and the relevant distribution of faults across programming languages.
  \item an evaluation of the most used languages used across the top 16 most popular domains identified by SourceForge 13'.
  \item an investigation into the languages that achieve the highest/lowest ratios of hotfix per version update.
  \item research into the programming topics that experience the highest/lowest numbers of hotfix per version increase.
  \item an investigation into the following relationships between languages: strongly-typed vs weakly-typed, static vs dynamic and scripting vs procedural to try to determine what group of languages is the most reliable.
  \item a foundation for future research in the area of identifying language-specific hotfixes.
\end{itemize}

The rest of the paper is organised as follows: Related work is discussed in section 2. Section 3 explains our empirical study with 4 research questions. In Section 4, the results are presented and discussed and followed by our conclusion and future work in Section 5.

\section{Related Work}
Many researchers have explored various aspects of defects in software through the use of versioning system history logs. We have also decided to follow this approach due to its openness and accessibility. This method is also very widely adopted amongst the MSR community \cite{Hattori2008}. Our investigation focused primarily on two research aspects and involved exploring existing work related to: 1) identifying hotfix commits and 2) comparison of programming languages' stability.


\subsection{Identifying hotfix commits}
The first major problem we had to solve was identifying actual hotfix commits. Due to Boa's limitations, we were not able to find commit history from hotfix branch. We were only capable of collecting information with regard to commits performed on the main branch. Therefore, we reviewed various papers to identify the most appropriate method of determining the type of the commit to extract only the ones related to a bug correction.\par

A large number of the reports that we studied implemented a very simple approach. They focused on the commit message as the main mechanism to try to identify whether a commit was of a corrective type~\cite{Hattori2008, Ratzinger2008, Hindle2008}. They simply investigated if the string of text contained particular key words, such as: ``bug'', ``fix'', ``issue'', ``error'', ``broke'', etc.\par

Some authors proposed a solution that involved a slightly more complex procedure including a syntactic analysis of the commit messages. Their algorithm consisted of multiple steps: 1) normalization of the data (removing non-alphanumeric characters), 2) word frequency analysis including a classification of the most common words ( e.g. ``for'', ``code'', ``to'', ``the'', etc. ) as being neutral, 3) implementing a keyword clustering algorithm to classify the commits \cite{Mockus2000}.\par

Another approach to identify bug fix commits involved performing a classification based on the size of the commits \cite{Hattori2008,Mockus2000,Hindle2008}. The size of a commit can be measured based on the number of lines or the amount of files that were changed. According to these authors, large commits tend to improve code quality or add new features, while small ones are considered to be corrective - targeted at fixing bugs in the project. Some researchers proposed a further fine-grained size classification depending on the number of files changed per commit \cite{Hattori2008}. They defined the following categories: 1) tiny (1 to 5 changed files), 2) small (6 to 25 files), 3) medium (26 to 125 files), large (126 up ). The scientists further argued that most of the commits tend to be tiny (in 80\% of the cases). This implies that bug fix commits occur very often as corrective commits tend to be small ones.\par

We also looked at the time factor with regard to commits. According to some authors, issues in critical components (e.g. errors in operating system drivers, security errors) tend to be fixed relatively quickly when discovered \cite{Livshits2005}. In \cite{Mockus2000}, the researchers argue that the time between changes will depend on the type of alteration. Furthermore, they hold the view that corrective changes require the least amount of time to be completed as often they have to be performed promptly regardless of the complexity of the task.\par

In terms of the time aspect, there was another paper which investigated the duration between the moment when a change is applied to a specific branch and the moment the new code was merged into the trunk \cite{Williams2008}. However, they explored branches in general without focusing specifically on bugfix branches. Some other scientists were trying to explain people's behaviour during commits by investigating their commit frequency distribution. As a result, they discovered that on average the time between commits was 3.206 days and half of all the contributors spent less than 13.78 hours between 50\% of their commits \cite{Kolassa2013}.\par

Another solution related to identifying defects in software focused on analysing commit metadata to discover design smells and predict where a break in different areas of the code may appear every time a change is performed \cite{Oliva2013}. However, they did not make their investigation language-specific as opposed to what we propose in our paper.\par

Other scientists tried to explore the logical coupling of commits as a way of identifying potential software defects. They aimed at developing a method to reduce the number of bugs in the code during the development stage so that less fix commits will be needed in the future \cite{Steff2012}.\par

A further interesting research that we found was related to the use of separate bug fix branches. According to the study performed by Phillips et al. \cite{Phillips2011}, 41\% of the tested projects had incorporated an isolated bug fix branch.

\subsection{Comparison of programming languages' stability}
Throughout our investigation we discovered that there were a lot of authors that tried to utilize commit data to try to propose good engineering practices and find a way to determine code quality \cite{Agrawal2015}. Furthermore, there were several papers that compared and contrasted the performance of different languages, which led to our second research topic. We explored these existing scientific studies to see what techniques authors had implemented. We also analysed the hypotheses these papers proposed to increase the scope of our project.\par

Our specific point of interest was related to different software topics and programming languages to discover how number of hotfixes per topic depend on the language developers use.\par

There were researchers that focused on the effect of programming languages on software quality \cite{Ray2014}. The authors based performance on the rate of defect occurrences in various languages. The study was carried out on a huge dataset from GitHub (1.5 million commits). To evaluate the effect that programming languages have on software quality, the authors implemented the following strategy. First, they grouped languages according to their type, i.e. strongly vs weakly typed. They also used mathematical methods to triangulate and produce statistics for how group size, project history and number of contributors affect the number of defects. In order to achieve more representative results, they took a sample of 17 languages and extracted the top 50 projects per language. The results showed how defect-prone languages are compared to others. This was done by taking a grand mean and then comparing each language to the grand mean. Languages were then grouped, a mean was taken and then types of languages were once again compared to a grand mean - e.g. functional-strong-static languages compared to functional-dynamic-strong languages. This research focused on classifying bugs based on their type as opposed to our investigation, where we focused specifically on the distribution of hotfixes across programming languages.\par

In \cite{Phipps1999}, Java and C++ were evaluated by comparing bug rates. In this case, there were two definitions for bugs - a defect and a bug itself. A defect included issues such as syntax errors, logical errors and changes in variable name. A bug itself was only defined to be a problem detected during testing or deployment. The mean number of defects and bugs, averaged over a number of projects, was compared between both languages per 1000 lines of code. The scientists also compared observed defects and bugs per hour as well as the time taken to fix defects in both languages. Productivity was also measured by analysing number of lines written per second.\par

There were researchers that used slightly different parameters to compare programming languages, these included program length, programming effort, run-time efficiency, memory consumption, and reliability \cite{Prechelt2000}.\par

An interesting paper that we reviewed analysed memory usage and speed of execution of three different bioinformatics methods for six distinct programming languages \cite{Fourment2008}. This was especially pertinent as bioinformatics applications have huge datasets and computational time taken is not trivial, hence optimising speed is desirable.\par

Another group of authors compared 8 different languages by conciseness, size of executables, running time, memory usage and failure proneness \cite{Nanz2015}. They argued that conciseness is an important factor as a more compact language can help write a program with fewer bugs.


\section{Empirical Study}

% \begin{table}
% \centering
% \resizebox{\columnwidth}{!}{%
% \begin{tabular}{r|lll}
% \multicolumn{1}{r}{}
% & \multicolumn{1}{l}{Heading 1}
% & \multicolumn{1}{l}{Heading 2}
% & \multicolumn{1}{l}{Heading 3} \\ \cline{2-4}
% Row 1 & Cell 1,1 & Cell 1,2 & Cell 1,3 \\
% Row 2 & Cell 2,1 & Cell 2,2 & Cell 2,3
% \end{tabular}%
% }
% \end{table}

Our work was inspired by the previous research paper \cite{Ray2014}, where authors compared the number of bug commits to the total number of commits across 17 languages and 850 different projects. Our contribution primarily involved investigation of the fault-proneness of different languages based on the newly defined criteria - hotfixes per version update. 

\subsection{Dataset}
The SourceForge13 Dataset is selected for this study. It consists of \FIXME{[totalnumberofprojects]} projects, selected from the top 16 most popular project types in the SourceForge dataset. That enables us to differentiate results for the different kinds of projects.

\subsection{Research Questions}
\noindent Our study aimed to answer the following research questions:

\noindent\textbf{RQ1 (Language Popularity):} What languages are the most widely used across the top 16 most popular categories?

\noindent\textbf{RQ2 (Hotfixes vs Project Categories):} What project categories achieve the highest/lowest number of hotfixes per version update?

\noindent\textbf{RQ3 (Hotfixes vs Languages):} What languages achieve the highest/lowest number of hotfixes per version update?

\noindent\textbf{RQ4 (Hotfixes vs Language Types):} Are there any relationship between the occurrences of hotfixes and the type of the programming language?

\begin{table*}[!htbp]
	\centering
	\small
	\begin{tabular}{l c c c r r r r r r r r r r r r r r r r r}
		Language & $\mathcal{P}$ & $\mathcal{C}$ & $\mathcal{T}$ & 
		\rotatebox{90}{Databases} & \rotatebox{90}{Systems Administration} & 
		\rotatebox{90}{Dynamic Content} & 
		\rotatebox{90}{Front Ends} & 
		\rotatebox{90}{Site Management} & 
		\rotatebox{90}{Role Playing} & \rotatebox{90}{Games} & \rotatebox{90}{Security} &
		\rotatebox{90}{Internet} & \rotatebox{90}{Software Development} & 
		\rotatebox{90}{Testing} & 
		\rotatebox{90}{Build Tools} & 
		\rotatebox{90}{Code Generators}&
		\rotatebox{90}{Communications} & \rotatebox{90}{Education} & \rotatebox{90}{Frameworks} &
		\rotatebox{90}{Average} \\
		\hline
		Java & P & S & S & 0.25 & 0.15 & 0.22 & 0.08 & 0.28 & 0.32 & 0.07 & 0.22 & 0.38 & 0.17 & 0.23 & 0.10 & 0.06 & 0.09 & 0.10 & 0.09 & \textbf{0.176} \\
		C\#  & P & S & S & -&0.20&-&-&0.30&0.89&0.14&0.27&0.60& 0.08 & - & 0.07 & - & 0.57 & - & 0.27 & \textbf{0.339} \\
	    C++  & P & S & W & 0.24&0.12&0.26&0.06&-&0.51&0.35&0.31&0.20&0.26&0.27&0.35&0.73&0.08&0.12&0.43 & \textbf{0.286} \\ 
		C  & P & S & W & 0.29&0.24&0.29&0.33&0.43&0.56&0.39&0.05&0.03&0.34&0.27&0.33&0.73&0.07&0.23& - & \textbf{0.305}\\
		PHP  & S & D & W & 0.27&0.23&0.27&0.28&0.24&-&-&0.23&0.10&0.38&-&-&0.14&-&0.67&0.31&\textbf{0.282} \\		
		Perl  & S & D & S & 0.17 & 0.05 & 0.22 & 0.44 & - & 0.41 & - & 0.06 & 0.13&0.24&-&-&-&0.17&0.19&0.32 & \textbf{0.218} \\
		Python	& S & D & S & 0.27	& 0.24	& 0.36	& -	& - & 0.40 & 0.42	& 0.13	& -	& 0.41	& -	& 0.33	& -	& 0.14	& 0.22	& 0.36	& \textbf{0.298} \\
		JavaScript  & S & D & W & 0.26 & - & 0.27 & - & 0.17 & - & 0.17 & 0.23 & - & - & - & - & - & - & 0.18 & 0.18 & \textbf{0.209} \\
	
		\hline
		Average & &  & & \textbf{0.25}&\textbf{0.17}&\textbf{0.28}&\textbf{0.28}&\textbf{0.28}&\textbf{0.47}&\textbf{0.33}&\textbf{0.17}&\textbf{0.12}&\textbf{0.32}&\textbf{0.27}&\textbf{0.34}&\textbf{0.53}&\textbf{0.11}&\textbf{0.27}&\textbf{0.32} \\
		\hline
	\end{tabular}
	\caption{Ration of hotfix per version update of 8 common languages and their 16 respective domains. $\mathcal{P}$, $\mathcal{C}$, and $\mathcal{T}$ represent the classes of the language. $\mathcal{P}$ stands for Programming Paradigm class (P=Procedural, S=Scripting). $\mathcal{C}$ is for Compilation class (S=Static, D=Dynamic), and $\mathcal{T}$ is for Type class (W=weakly-typed, S=Strongly-typed).}
	\label{tab:1}
\end{table*}

\section{Results}
We performed an empirical study on the SourceForge13 dataset aiming to find the answers for our 4 research questions. The findings of each research question is explained below.
\subsection{RQ1: Language Popularity}
Our investigation on the most widely used languages across the top 16 most popular categories revealed that there are surprisingly few such languages. We identified only 8 widely used languages: Java, C, C++, Perl, Python, PHP, C\#, and JavaScript. The criteria of choosing the languages is based on whether the language presents in most of the categories. We found that there is a significant drop in the amount of occurrence after JavaScript.  %One would expect JavaScript to appear in the list. However, it is present only in 7 categories and hence is ignored from the list. This may be due to the fact that JavaScript is mainly used only for website development while our 16 popular categories span over various aspects of software. 

From Table \ref{tab:1}, \textbf{the most widely used languages from the list is Java which appears in all the 16 categories.} C, and C++ share the second place with occurrence in 15 out of 16 categories. While C++ is not present in Site Management, C is not present in Frameworks. The rest of the languages in the list are present in 10 categories on average. However, it is interesting to note that the majority of these languages are not used in Testing, which contains only Java, C, and C++. Similarity, only a few languages appears in Code Generators and Site Management categories. The observations made are reasonable since using these languages in such categories is very uncommon. \FIXME{Citation here?}

\subsection{RQ2: Hotfixes vs Project Categories}
Next, we investigate which categories achieve the highest or lowest numbers of hotfixes per version update. We proceed to analyse why certain categories might have low numbers of hotfixes and vice versa.
    
In Table \ref{tab:1}, if we look at the average number of hotfixes per category we see that \textbf{the Internet, Security, Communications, and Systems Administration have the lowest average number of hot fixes per update} out of the entire group of categories. The categories with \textbf{the highest average numbers of hotfixes per version update are Code Generators and Role playing} which have 0.53 and 0.47 average hot fixes per update respectively. Both these values are much higher than the next highest (0.34).

With regards to those with lower average hot fixes per update, there is a reasonable explanation. Applications under the categories as Security, Internet, Communications and Systems administration are all information critical, which means they have critical assets that need to be protected in order to maintain customer's privacy and trust. For example, with Communications applications it is vital that security is implemented in such a way that an adversary cannot intercept or obtain messages only intended for the receiver. This means that applications that fall under such categories should be extensively tested before deployment to reduce the chance of failure. This results in less hot fix being needed after deployment.

Another supporting example is the usage of C in Internet and Security categories has an incredibly low hot fix average (0.03 and 0.05 respectively). This could be down to fully comprehensive testing techniques, but could also be attributed to the fact that within those categories, the language has been used for an extensive period and hence there is a lot of experience with a variety of different bugs, meaning they are less likely to occur during development as experienced programmers are less likely to make those particular mistakes in the first place. This could also be the reason the projects written in Perl have a very low hot fix average for all four categories.

Role Playing applications have a high average number of hot fixes compared to other projects that fall under the Games category. It is possible that the Games category is a general category. It is a lot simpler than those that fall under the role playing category which is why bugs might be more prominent as a result of increased complexity of the game. Moreover, usually this kind of game contains huge amount of configurations of characters, and items in the game. It is common to release a few hotfixes after releasing a new version due to fixing these configurations. We can obviously see that all languages in Role Playing category have high average numbers of hot fixes per update.

On the other hand, for Code generators, C and C++, have very high hot fix averages while  PHP and Java are the lowest. One can speculate that PHP and Java are established languages for writing code generators and due to this engender low hot fix averages. C and C++ might share syntactic details and semantic structure that would result in a similar hot fix average between them.

\subsection{RQ3: Hotfixes vs Languages}
Language comparison based on the average hotfix ratio over all the categories in Table 1 has shown that \textbf{Java has appeared to have the least average ratio of hotfixes per version update (0.176)} followed by JavaScript (0.209) and Perl (0.218). However, all the mentioned languages show marginally better results with 8\% difference comparing to other common languages taken under analysis: PHP(0.282), C++(0.286), C (0.305) and C\#(0.339). \textbf{C\# has the highest average ratio of hotfixes among all the languages.}

\subsection{RQ4: Hotfixes vs Language Types}
The interesting research question is comparing the type of each programming language by their average ratio of hotfixes. The classification in Table \ref{tab:1} is inspired by \cite{Ray2014}, where languages were compared based on the number of bug fixes in each language and were divided into three main Classes. The first class (Programming Paradigm) has the following categories: Procedural, Functional, Scripting. The second class is Compilation containing Static and Dynamic type. The final one, Type Class, separates languages into Strongly-typed and Weakly-typed. According to \cite{Ray2014}, strongly-typed languages experienced fewer bug fixes than its counterpart, weakly-typed. Static programming languages required fewer corrective commits than dynamic ones. And finally, functional languages encountered fewer bug fixes than procedural ones.

Due to our criteria of selecting the languages, the chosen 8 languages are all either procedural or scripting, but not functional. Most probably this was due to the fact that functional languages are limited to specific use cases and as such are not widely popular across different domains. \FIXME{citation here?} Consequently, we compare only Procedural and Scripting languages.

\subsubsection*{Programming Paradigm: Procedural vs Scripting}
From Table \ref{tab:1}, there is no clear trend or pattern of the hotfix ratio among Procedural (Java, C, C\#, C++) and Scripting (Perl, Python, PHP) across all the categories. This is possibly because the number of data points (languages) was too low to observe such behaviours and make any firm conclusions. %It was always the case that at least one of the languages would not follow a pattern. For some categories, the comparison would consider only one language from a certain category.

\subsubsection*{Compilation: Static vs Dynamic}
In this class, Static (Java, C, C\#, C++) and Dynamic (Perl, Python, PHP) languages led to the same conclusions of no clear different of hotfix ratio observed between the two sets as can be seen from Table \ref{tab:1}. The reason was because the languages in these two categories coincided with the languages in the first two classes (Procedural and Scripting).

\subsubsection*{Type: Strongly-type vs Weakly-typed}
The third class compares strongly-typed (Java, C\#, Perl, Python) and weakly-typed (C, C++, PHP) languages. Similarly to the previous two classes, we encountered no clear pattern across the categories. Furthermore, we found the same problem of having comparisons against only one member of a language class. Solely in the Communications category, we were able to detect some pattern, where strongly-typed languages performed slightly better. However, there were only two languages labelled as weak. Therefore, we cannot reach any satisfying conclusions based on this finding.

Overall, \textbf{we could not identify any convincing patterns based on the class comparison between languages} mostly due to the low number of selected programming languages.

\subsection{Limitations}
There were several limitations that we encountered throughout our study. First, we were able to analyse only a small part of all available projects because most of the developers did not support a versioning system, i.e. no bump commits could be identified.

%Therefore, we may have introduced some false positives or failed to account for actual hotfixes.
Moreover, we could not extract hotfix branches, only the overall commits on the trunk were available and thus we used our own parameters to try to distinguish hotfix commits. The way we detected bump commits was by writing a regular expression to match words like:  ``bump'', ``version'', ``update'' in an appropriate order. Thus, we may generate false positives by including version updates that are not real hotfixes, and also false negatives by excluding version updates had they been indicated differently.

Most open source projects are not under the stress of releasing as opposed to commercial projects. Consequently, they do not release very often and/or do not support versioning.

There is a low amount of languages satisfied our predicate (being present in at least one half of all evaluated topics), which did not allow us to state any conclusive results. In addition, with a limited number of programming topics was used (16 in total), we are restricted to observe distinguishable patterns.

Lastly, only one dataset was used to evaluate our hypothesis: SourceForge 2013. Hence, this results may not be generalised.

\section{Conclusions and Future Work}
We have presented a study in detecting the fault-proneness of programming languages through hotfix commits, a topic that has not been investigated so far. As part of our research we performed a topic-specific analysis across 16 programming domains (categories) to detect the most popular languages used in the evaluated  projects. We investigated languages and topics that achieved highest/lowest ratios of hotfix per version update and tried to identify patterns based on the language classification presented in \cite{Ray2014}. We used Boa to mine repositories in the SourceForge 2013's dataset. %Our goal was to verify whether the general relationships between languages' reliability holds with regard to hotfixes.  
We identified 8 languages as being the most widely used across different domains including Java, C, C++, Perl, Python, PHP, C\#, and JavaScript respectively. We discovered that Java achieves the least number of hotfixes per version update (0.176), while C\# requiring the highest amount of hotfixes per version increase (0.339). From the different categories, we observed that Security, Internet and Communications is the most reliable (i.e. requiring least amount of hotfixes per version update) while Role Playing and Code Generation required the highest number of corrections by hotfixes. Finally, according to the language classes, we were not able to identify any conclusive patterns.

We believe that there are still aspects to be improved in our study. In particular, more datasets could be used, including Github (provided that projects are categorised into topics), and more reliable and accurate method to identify bump commits. Instead of using data from SourceForge, real commercial projects which have been open-sourced can be investigated to enhance the quality of our data. In such projects, releases should occur more frequently and thus, there is a higher probability of identifying more hotfixes.

Finally, we hope that our paper lays a good foundation for future research to be done in this area. We expect that the results we obtained will help developers and project managers improve their planning and understand what are the most appropriate languages for their specific topic or purpose.

\bibliographystyle{abbrv}
\bibliography{hotfixes} 

% that's all folks
\end{document}


