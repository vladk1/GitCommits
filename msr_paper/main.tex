% \documentclass[conference]{IEEEtran}

% \begin{document}
% \title{An Important Conference Contribution}

% \author{\IEEEauthorblockN{Author One\IEEEauthorrefmark{1},
% Author Two\IEEEauthorrefmark{2}, Author Three\IEEEauthorrefmark{3} and
% Author Four\IEEEauthorrefmark{4}}
% \IEEEauthorblockA{Department of Whatever,
% Whichever University\\
% Wherever\\
% Email: \IEEEauthorrefmark{1}author.one@add.on.net,
% \IEEEauthorrefmark{2}author.two@add.on.net,
% \IEEEauthorrefmark{3}author.three@add.on.net,
% \IEEEauthorrefmark{4}author.four@add.on.net}}
% \maketitle

% \end{document}

\documentclass{sig-alternate}
\usepackage{blindtext, graphicx}
\usepackage{fancyhdr} 
\usepackage{txfonts}
\usepackage{lipsum}
\usepackage{booktabs}

\graphicspath{ {images/} }

\newcommand\FIXME[1]{\textbf{FIXME: #1}}
%\fancyhf{}
%\cfoot{\thepage}
%\pagestyle{fancy}

% *** GRAPHICS RELATED PACKAGES ***
%
%\ifCLASSINFOpdf
  % \usepackage[pdftex]{graphicx}
  % declare the path(s) where your graphic files are
  % \graphicspath{{../pdf/}{../jpeg/}}
  % and their extensions so you won't have to specify these with
  % every instance of \includegraphics
  % \DeclareGraphicsExtensions{.pdf,.jpeg,.png}
%\else
  % or other class option (dvipsone, dvipdf, if not using dvips). graphicx
  % will default to the driver specified in the system graphics.cfg if no
  % driver is specified.
  % \usepackage[dvips]{graphicx}
  % declare the path(s) where your graphic files are
  % \graphicspath{{../eps/}}
  % and their extensions so you won't have to specify these with
  % every instance of \includegraphics
  % \DeclareGraphicsExtensions{.eps}
%\fi

% correct bad hyphenation here
%\hyphenation{op-tical net-works semi-conduc-tor}

%\pagenumbering{arabic}
\begin{document}
\conferenceinfo{MSR}{'16 Austin, Texas USA}
%
% paper title
% can use linebreaks \\ within to get better formatting as desired

\title{Investigating Fault-Proneness Through Hotfix Commits:\\A Research in the Light of Different Programming Languages}
\numberofauthors{4}
\author{
  Martin Lazarov, Ismaa'eel Ataullah, Vladyslav Kolesnyk, Veselin Pavlov\\
  \affaddr{University College London}\\
  \affaddr{London, UK}
}


%\title{Investigating Fault-Proneness Through Hotfix Commits: A Research in the Light of Different Programming Languages}

% author names and affiliations
% use a multiple column layout for up to three different
% affiliations
%\author{\IEEEauthorblockN{Martin Lazarov\IEEEauthorrefmark{1},
%Ismaa'eel Ataullah\IEEEauthorrefmark{2}, Vladyslav Kolesnyk\IEEEauthorrefmark{3},
%Veselin Pavlov\IEEEauthorrefmark{4}}
%\IEEEauthorblockA{Department of Computer Science,
%UCL\\
%Email: \IEEEauthorrefmark{1}martin.lazarov.12@ucl.ac.uk,
%\IEEEauthorrefmark{2}ismaaeel.ataullah.12@ucl.ac.uk,
%\IEEEauthorrefmark{3}vladyslav.kolesnyk.12@ucl.ac.uk,
%\IEEEauthorrefmark{4}veselin.pavlov.15@ucl.ac.uk}}

% make the title area
\maketitle

\begin{abstract}
%\boldmath
Choosing the most suitable tools and programming languages is a very challenging task for every project manager and developer. Often the success of a software project is determined by the stability of the system after a release. In this paper, we propose analysing hotfix commits in various development areas as a means of indicating what languages tend to produce the least amount of problems after software has been published. We have focused on parsing metadata from projects hosted on SourceForge related to five specific topics (``software development'', ``databases'', ``security'', ``games'', ``front-ends'') and have performed an estimation of the amount of deployed hotfixes to compare the fault-proneness of various languages. 
\end{abstract}

% A category with the (minimum) three required fields
\category{D.2.7}{Software Engineering}{\FIXME{}}
%A category including the fourth, optional field follows...

%\terms{Experimentation, Measurement, Languages, Performance}

\keywords{Version Control, hotfixes, SourceForge, commits metadata}

%\begin{IEEEkeywords}
%Version Control, hotfixes, SourceForge, commits metadata.
%\end{IEEEkeywords}

\section{Introduction}
Planning and resource allocation are arguably the most crucial components of any software project. Our research was motivated by the fact that very often, in the planning phase project managers neglect the importance of hotfixes - which becomes essential once a project is deployed.

Generally, a hotfix is related to correcting a critical problem which prevents a particular software product from performing as expected. This could mean a total or partial loss of service, a major security breach that has to be patched instantly, etc. As a result, such a fix is done outside of normal development and testing procedures. This characteristic of hotfixes implies that rapid development under urgent circumstances is usually required. Therefore, it can be often the case that lots of additional people and resources need to be allocated quickly for the correction, which leads to a significant increase in the cost of the overall product.

Another aspect that makes hotfixes so important is their unpredictability which further prevents teams from planning in advance for such corrections into their development cycle. Moreover, hotfixes are sometimes delayed on purpose because people are afraid of the risk of applying the fix. All of these factors contribute to increasing the uncertainty and stress in managers and that can prevent them from taking the most appropriate action when a problem in production has to be resolved promptly.

As there are no clear definitions that specify what a hotfix is, according to our research and observations from the open-source projects, a hotfix can be defined as: ``a corrective commit deployed within 1 day, following a project's version update to fix critical issues in the release''.\par

With our research we aim to help project managers and developers make a more informed decision when deciding what languages to use by comparing the number of hotfixes per programming language. To provide a better picture we also look at hotfixes per specific software topics. 

We believe that the results from our research can provide insights into the average number of issues per project with a focus on different programming topics and the relevant languages used. Consequently, project managers will be able to make a more informed decision with regard to the most appropriate programming languages they could use to try to achieve a minimum number of problems in production. This will improve software engineering practices and the overall project planning activity. Furthermore, it will help optimise the allocation of resources.
The contributions of this paper are:

\begin{itemize}
  \item parameters to determine most probable hotfix commits in the repository.
  \item a quantitative analysis identifying fault-proneness of specific programming topics and the relevant distribution of faults across programming languages.
  \item an evaluation of the most used languages used across the top 16 most popular domains identified by SourceForge 13'.
  \item an investigation into the languages that achieve the highest/lowest ratios of hotfix per version update.
  \item research into the programming topics that experience the highest/lowest numbers of hotfix per version increase.
  \item an investigation into the following relationships between languages: strongly-typed vs weakly-typed, static vs dynamic and scripting vs procedural to try to determine what group of languages is the most reliable.
  \item a foundation for future research in the area of identifying language-specific hotfixes.
\end{itemize}
\par
\textbf{Structure of the paper.}\\
We discuss Related Work in section 2. The next segment discusses our Solution. Following from that, an analysis of the gathered results is presented. Finally, we review our approach and findings, and examine Future Work.


\section{Related Work}
Many researchers have explored various aspects of defects in software through the use of versioning system history logs. We have also decided to follow this approach due to its openness and accessibility. This method is also very widely adopted amongst the MSR community \cite{Hattori2008}. Our investigation focused primarily on two research aspects and involved exploring existing work related to: 1) identifying hotfix commits and 2) comparison of programming languages' stability.


\subsection{Identifying hotfix commits}
The first major problem we had to solve was identifying actual hotfix commits. Due to Boa's limitations, we were not able to find commit history from hotfix branch. We were only capable of collecting information with regard to commits performed on the main branch. Therefore, we reviewed various papers to identify the most appropriate method of determining the type of the commit to extract only the ones related to a bug correction.\par

A large number of the reports that we studied implemented a very simple approach. They focused on the commit message as the main mechanism to try to identify whether a commit was of a corrective type~\cite{Hattori2008, Ratzinger2008, Hindle2008}. They simply investigated if the string of text contained particular key words, such as: ``bug'', ``fix'', ``issue'', ``error'', ``broke'', etc.\par

Some authors proposed a solution that involved a slightly more complex procedure including a syntactic analysis of the commit messages. Their algorithm consisted of multiple steps: 1) normalization of the data (removing non-alphanumeric characters), 2) word frequency analysis including a classification of the most common words ( e.g. ``for'', ``code'', ``to'', ``the'', etc. ) as being neutral, 3) implementing a keyword clustering algorithm to classify the commits \cite{Mockus2000}.\par

Another approach to identify bug fix commits involved performing a classification based on the size of the commits \cite{Hattori2008,Mockus2000,Hindle2008}. The size of a commit can be measured based on the number of lines or the amount of files that were changed. According to these authors, large commits tend to improve code quality or add new features, while small ones are considered to be corrective - targeted at fixing bugs in the project. Some researchers proposed a further fine-grained size classification depending on the number of files changed per commit \cite{Hattori2008}. They defined the following categories: 1) tiny (1 to 5 changed files), 2) small (6 to 25 files), 3) medium (26 to 125 files), large (126 up ). The scientists further argued that most of the commits tend to be tiny (in 80\% of the cases). This implies that bug fix commits occur very often as corrective commits tend to be small ones.\par

We also looked at the time factor with regard to commits. According to some authors, issues in critical components (e.g. errors in operating system drivers, security errors) tend to be fixed relatively quickly when discovered \cite{Livshits2005}. In \cite{Mockus2000}, the researchers argue that the time between changes will depend on the type of alteration. Furthermore, they hold the view that corrective changes require the least amount of time to be completed as often they have to be performed promptly regardless of the complexity of the task.\par

In terms of the time aspect, there was another paper which investigated the duration between the moment when a change is applied to a specific branch and the moment the new code was merged into the trunk \cite{Williams2008}. However, they explored branches in general without focusing specifically on bugfix branches. Some other scientists were trying to explain people's behaviour during commits by investigating their commit frequency distribution. As a result, they discovered that on average the time between commits was 3.206 days and half of all the contributors spent less than 13.78 hours between 50\% of their commits \cite{Kolassa2013}.\par

Another solution related to identifying defects in software focused on analysing commit metadata to discover design smells and predict where a break in different areas of the code may appear every time a change is performed \cite{Oliva2013}. However, they did not make their investigation language-specific as opposed to what we propose in our paper.\par

Other scientists tried to explore the logical coupling of commits as a way of identifying potential software defects. They aimed at developing a method to reduce the number of bugs in the code during the development stage so that less fix commits will be needed in the future \cite{Steff2012}.\par

A further interesting research that we found was related to the use of separate bug fix branches. According to the study performed by Phillips et al. \cite{Phillips2011}, 41\% of the tested projects had incorporated an isolated bug fix branch.

\subsection{Comparison of programming languages' stability}
Throughout our investigation we discovered that there were a lot of authors that tried to utilize commit data to try to propose good engineering practices and find a way to determine code quality \cite{Agrawal2015}. Furthermore, there were several papers that compared and contrasted the performance of different languages, which led to our second research topic. We explored these existing scientific studies to see what techniques authors had implemented. We also analysed the hypotheses these papers proposed to increase the scope of our project.\par

Our specific point of interest was related to different software topics and programming languages to discover how number of hotfixes per topic depend on the language developers use.\par

There were researchers that focused on the effect of programming languages on software quality \cite{Ray2014}. The authors based performance on the rate of defect occurrences in various languages. The study was carried out on a huge dataset from GitHub (1.5 million commits). To evaluate the effect that programming languages have on software quality, the authors implemented the following strategy. First, they grouped languages according to their type, i.e. strongly vs weakly typed. They also used mathematical methods to triangulate and produce statistics for how group size, project history and number of contributors affect the number of defects. In order to achieve more representative results, they took a sample of 17 languages and extracted the top 50 projects per language. The results showed how defect-prone languages are compared to others. This was done by taking a grand mean and then comparing each language to the grand mean. Languages were then grouped, a mean was taken and then types of languages were once again compared to a grand mean - e.g. functional-strong-static languages compared to functional-dynamic-strong languages. This research focused on classifying bugs based on their type as opposed to our investigation, where we focused specifically on the distribution of hotfixes across programming languages.\par

In \cite{Phipps1999}, Java and C++ were evaluated by comparing bug rates. In this case, there were two definitions for bugs - a defect and a bug itself. A defect included issues such as syntax errors, logical errors and changes in variable name. A bug itself was only defined to be a problem detected during testing or deployment. The mean number of defects and bugs, averaged over a number of projects, was compared between both languages per 1000 lines of code. The scientists also compared observed defects and bugs per hour as well as the time taken to fix defects in both languages. Productivity was also measured by analysing number of lines written per second.\par

There were researchers that used slightly different parameters to compare programming languages, these included program length, programming effort, run-time efficiency, memory consumption, and reliability \cite{Prechelt2000}.\par

An interesting paper that we reviewed analysed memory usage and speed of execution of three different bioinformatics methods for six distinct programming languages \cite{Fourment2008}. This was especially pertinent as bioinformatics applications have huge datasets and computational time taken is not trivial, hence optimising speed is desirable.\par

Another group of authors compared 8 different languages by conciseness, size of executables, running time, memory usage and failure proneness \cite{Nanz2015}. They argued that conciseness is an important factor as a more compact language can help write a program with fewer bugs.


\section{Solution}

% \begin{table}
% \centering
% \resizebox{\columnwidth}{!}{%
% \begin{tabular}{r|lll}
% \multicolumn{1}{r}{}
% & \multicolumn{1}{l}{Heading 1}
% & \multicolumn{1}{l}{Heading 2}
% & \multicolumn{1}{l}{Heading 3} \\ \cline{2-4}
% Row 1 & Cell 1,1 & Cell 1,2 & Cell 1,3 \\
% Row 2 & Cell 2,1 & Cell 2,2 & Cell 2,3
% \end{tabular}%
% }
% \end{table}



\begin{table*}[!htbp]
  \centering
  \begin{tabular}{cccccccccccccccccc}
    Language &
    \rotatebox{90}{Databases} & \rotatebox{90}{Systems Administration} & 
    \rotatebox{90}{Dynamic Content} & 
    \rotatebox{90}{Front Ends} & 
    \rotatebox{90}{Site Management} & 
    \rotatebox{90}{Role Playing} & \rotatebox{90}{Games} & \rotatebox{90}{Security} &
    \rotatebox{90}{Internet} & \rotatebox{90}{Software Development} & 
    \rotatebox{90}{Testing} & 
    \rotatebox{90}{Build Tools} & 
    \rotatebox{90}{Code Generators}&
    \rotatebox{90}{Communications} & \rotatebox{90}{Education} & \rotatebox{90}{Frameworks} &
    \rotatebox{90}{Total Average} \\
    \midrule
    Java & 0.25 & 0.15 & 0.22 & 0.08 & 0.28 & 0.32 & 0.07 & 0.22 & 0.38 & 0.17 & 0.23 & 0.10 & 0.06 & 0.09 & 0.10 & 0.09 & \textbf{0.176} \\
    Javascript & 0.26 & - & 0.27 & - & 0.17 & - & 0.17 & 0.23 & - & - & - & - & - & - & 0.18 & 0.18 & \textbf{0.209} \\
    Perl & 0.17 & 0.05 & 0.22 & 0.44 & - & 0.41 & - & 0.06 & 0.13&0.24&-&-&-&0.17&0.19&0.32 & \textbf{0.218} \\
    PHP & 0.27&0.23&0.27&0.28&0.24&-&-&0.23&0.10&0.38&-&-&0.14&-&0.67&0.31&\textbf{0.282} \\
    C++ &0.24&0.12&0.26&0.06&-&0.51&0.35&0.31&0.20&0.26&0.27&0.35&0.73&0.08&0.12&0.43 & \textbf{0.286} \\ 
    C &0.29&0.24&0.29&0.33&0.43&0.56&0.39&0.05&0.03&0.34&0.27&0.33&0.73&0.07&0.23& - & \textbf{0.305}\\
    C\# & -&0.20&-&-&0.30&0.89&0.14&0.27&0.60& 0.08 & - & 0.07 & - & 0.57 & - & 0.27 & \textbf{0.339} \\
    \bottomrule
    Total Average & 0.25&0.17&0.28&0.28&0.28&0.47&0.33&0.17&0.12&0.32&0.27&0.34&0.53&0.11&0.27&0.32
  \end{tabular}
  \caption{Hotfix per version update ratios for the extracted languages and their respective domains}
  \label{tab:1}
\end{table*}


Our work was inspired by the previous research paper \cite{Ray2014}, where authors compared the number of bug commits to the total number of commits across 17 languages and 850 different projects. Our contribution primarily involved investigation of the fault-proneness of different languages based on the newly defined criteria - hotfixes per version update. Our dataset consist from [totalnumberofprojects] projects, selected from the top 19 most popular project types in the SourceForge dataset.
That allows us to differentiate results for the different kinds of projects.\par
Specifically, our investigation aimed to evaluate the following research questions:

\textbf{1) What languages are the most widely used across the top 16 most popular categories?} (as identified by SourceForge13 Dataset) 

Research on the most widely used languages across the top 16 most popular categories showed that there are surprisingly few such languages. We identified only 6 widely used languages: C\#, Perl, Python, C, C++ and PHP. The criteria of choosing the languages is based on whether the language was present in at least one half of the categories. One would expect Javascript to appear in the list. However, it is present only in 7 categories which is less than 50\% of the categories. This may be due to the fact that Javascript is mainly used for website development. The two most widely used languages from the list are C and C++ which are present in 15 out of 16 categories. While C++ is not present in site management, C is not present in frameworks. The rest of the languages in the list are present on average in 10 categories. Overall, the majority of these languages were not present in testing, code generators and site management categories. The observations made are reasonable since using these languages in such categories is very uncommon.

\textbf{2) What categories achieve the highest/lowest number of hotfixes per version update?}
    
In this section we investigate which categories achieve the highest or lowest numbers of hotfixes per version update. We proceed to analyse why certain categories might have low numbers of hotfixes and why others might require higher numbers of hotfixes.

If we look at the average number of hotfixes per category we see that the Internet, Security, communications and systems administration have the lowest average number of hot fixes per update out of the entire group of categories. 

The categories with the highest average numbers of hotfixes per version update are Code Generation and Role playing which have 0.53 and 0.47 average hot fixes per update respectively. Both these values are much higher than the next highest (0.34) So we will only consider these two.

With regards to those with lower average hot fixes per update it is easy to see why. Applications under the categories as Security, Internet, Communications and systems administration are all information critical i.e. they have critical assets that need to be protected in order to maintain customer trust and keep the reputation of the creator intact. For example with communications applications it is vital that security is implemented in such a way that an adversary cannot intercept or obtain messages only intended for the receiver. This means that applications that fall under such categories should be extensively tested before deployment reducing the chance of a hot fix being needed after deployment.

If we look further into the results we can see that for example the usage of C in Internet and security categories has an incredibly low hot fix average (0.03 and 0.05 respectively). This could be down to fully comprehensive testing techniques, but could also be attributed to the fact that within those categories, the language has been used for an extensive period and hence there is a lot of experience with a variety of different bugs, meaning they are less likely to occur during development as experienced programmers are less likely to make those particular mistakes in the first place. This could also be the reason the projects written in Perl have a very low hot fix average for all four categories.

When considering why role playing applications have a high average number of hot fixes we need to try and speculate as to why the hot fix rate was so much higher than that of projects that fall under the games category. It is possible that the games category is a general category are a lot simpler than those that fall under the role playing category which is why bugs might be more prominent as a result of increased complexity of the game. If we look a little closer at the results for the role playing category it becomes clear that for all languages there is a high average number of hot fixes per update which supports our theory.

If we look at Code generators on the other hand we see that for two languages (C and C++) the hot fix average is very high and for two other languages (PHP and Java) it is very low. It is not easy to conclude why this might be the case, however one can speculate that PHP and Java are established languages for writing code generators and due to this engender low hot fix averages. C and C++ might share syntactic details and semantic structure that would result in a similar hot fix average between them.

\textbf{3) What languages achieve the highest/lowest number of hotfixes per version update?}

Language comparison based on the average hotfix ratio over all topics has shown that Java has appeared to have the least number of hotfixes per version update (0.18). Javascript (0.21) and Perl (0.22) were found to perform slightly worse with the hotfix ratio value being just around 3-4\% higher. However, all the mentioned languages showed marginally better results with 8\% difference comparing to other common languages taken under analysis - PHP(0.28), C++(0.29), Python(0.30), C(0.31) and C\#(0.34). 

\textbf{4) Are there any patterns based on the type of the programming language?}

The fourth research question that we decided to investigate was related to comparing the type of each programming language. The classification was inspired by Ray et al. [14], where languages were compared based on the number of bug fixes in each language and were divided into three main Classes. The first class was named ‘Programming Paradigm’ and it split languages into the following categories: Procedural, Functional, Scripting. The second criteria was Compilation Class, which yielded these types: Static and Dynamic. The final one, Type Class, separated languages into: Strongly-typed and weakly-typed.

According to the results that were achieved in the study [14], strongly-typed languages experienced fewer bug fixes than weakly-typed. Static programming languages required fewer corrective commits than dynamic ones. And finally, functional languages encountered fewer bug fixes than procedural ones.

Due to the fact that we selected to evaluate languages that appeared in at least 50% of the selected categories, we discovered only 6 languages and none of them was functional. Most probably this was due to the fact that functional languages are limited to specific use cases and as such are not widely popular across different domains. Consequently, we evaluated the relationship between Procedural and Scripting languages instead of Procedural against Functional.

Analysing the first language relationship - Procedural (Java, C, C\#, C++) vs Scripting (Perl, Python, PHP), we were not able to detect any patterns across the categories. The reason was that the number of languages was too low to make any firm conclusions. It was always the case that at least one of the languages would not follow a pattern or the comparison would consider only one language from a certain category.

Evaluating the second relationship between Static (Java, C, C\#, C++) and Dynamic (Perl, Python, PHP) languages led to the same conclusions. The reason was because the languages in these two categories coincided with the languages in the first two classes (Procedural and Scripting).

The third aspect that we investigated referred to comparing strongly-typed (Java, C\#, Perl, Python) and weakly-typed (C, C++, PHP) languages. Similarly to the first language relationship, we encountered that there was no clear pattern across the categories. Furthemore, we found the same problem of having comparisons against against only one member of a language class. Solely in the Communications category, we were able to detect some pattern, where weakly typed languages performed better. However, there were only two languages labelled as weak. Therefore, we cannot reach any satisfying conclusions based on this finding.


Overall, we could not identify any convincing patterns based on the class comparison between languages mostly due to the low number of extracted programming languages.

\subsection{Limitations.}
There were several limitations that we encountered throughout our research:
\begin{itemize}
  \item we were able to analyse only a small part of all available projects because most of the developers did not support a versioning system, i.e. no bump commits could be identified.
  \item we could not extract hotfix branches, only the overall commits on the trunk were available and thus we used our own parameters to try to distinguish hotfix commits. Therefore, we may have introduced some false positives or failed to account for actual hotfixes.
  \item the way we detected bump commits was by writing a regular expression to match words like:  ``bump'', ``version'', ``update'' in an appropriate order. Thus, we may have excluded version updates had they been indicated differently. 
  \item most open source projects are not under the stress of releasing as opposed to commercial projects. Consequently, they do not release very often and/or do not support versioning.
  \item a limited number of programming topics was used (16 in total), which did not allow us to observe distinguishable patterns.
  \item a low amount of languages satisfied our predicate (being present in at least one half of all evaluated topics), which did not allow us to state any conclusive results.
  \item only one dataset was used to evaluate our hypothesis - SourceForge 2013.
\end{itemize}

\section{Conclusions and Future Work}
We have presented a study in detecting the fault-proneness of programming languages through hotfix commits - a topic that has not been investigated so far. As part of our research we performed a topic-specific analysis across 16 programming domains to detect the most popular languages used in the evaluated  projects. We investigated languages and topics that achieved highest/lowest ratios of hotfix per version update and tried to identify patterns based on the language classification. The tool that we created was written in Boa and mined repositories from SourceForge 2013's dataset. Our goal was to verify whether the general relationships between languages' reliability holds with regard to hotfixes.  
Our results identified 6 languages as being the most widely used across different domains - C\#, Perl, Python, C, C++ and PHP. We discovered that Java achieves the least number of hotfixes per version update (0.18), while C\# requiring most amount of hotfixes per version increase (0.34). From the different domains, we recognized: Security, Internet and Communications as being the most reliable (i.e. requiring least amount of hotfixes per version update) while Role Playing and Code Generation required highest number of corrections. Regarding the comparison between language classes, we were not able to identify any conclusive patterns.\par
There are few aspects that could be improved as Future Work. In particular, more datasets could be used, including Github (provided that projects are categorised into topics). Further improvements may be done to identify bump commits more accurately. Recently, a few commercial projects have been open-sourced and as a result, we could use them to enhance the quality of our data. In such projects, releases will have occurred more frequently and thus, there is a higher probability of identifying more hotfixes.\par
Finally, we believe that our paper provides a good foundation for future research to be done in this topic. We expect that the results we obtained will help developers and project managers improve their planning and understand what the most appropriate languages are for their specific topic.


\bibliographystyle{abbrv}
\bibliography{hotfixes} 

% An example of a floating table. Note that, for IEEE style tables, the 
% \caption command should come BEFORE the table. Table text will default to
% \footnotesize as IEEE normally uses this smaller font for tables.
% The \label must come after \caption as always.
%
%\begin{table}[!t]
%% increase table row spacing, adjust to taste
%\renewcommand{\arraystretch}{1.3}
% if using array.sty, it might be a good idea to tweak the value of
% \extrarowheight as needed to properly center the text within the cells
%\caption{An Example of a Table}
%\label{table_example}
%\centering
%% Some packages, such as MDW tools, offer better commands for making tables
%% than the plain LaTeX2e tabular which is used here.
%\begin{tabular}{|c||c|}
%\hline
%One & Two\\
%\hline
%Three & Four\\
%\hline
%\end{tabular}
%\end{table}




% if have a single appendix:
%\appendix[Proof of the Zonklar Equations]
% or
%\appendix  % for no appendix heading
% do not use \section anymore after \appendix, only \section*
% is possibly needed

% use appendices with more than one appendix
% then use \section to start each appendix
% you must declare a \section before using any
% \subsection or using \label (\appendices by itself
% starts a section numbered zero.)
%



% Can use something like this to put references on a page
% by themselves when using endfloat and the captionsoff option.
%\ifCLASSOPTIONcaptionsoff
%  \newpage
%\fi



% trigger a \newpage just before the given reference
% number - used to balance the columns on the last page
% adjust value as needed - may need to be readjusted if
% the document is modified later
%\IEEEtriggeratref{8}
% The "triggered" command can be changed if desired:
%\IEEEtriggercmd{\enlargethispage{-5in}}

% references section

% can use a bibliography generated by BibTeX as a .bbl file
% BibTeX documentation can be easily obtained at:
% http://www.ctan.org/tex-archive/biblio/bibtex/contrib/doc/
% The IEEEtran BibTeX style support page is at:
% http://www.michaelshell.org/tex/ieeetran/bibtex/
%\bibliographystyle{IEEEtran}
% argument is your BibTeX string definitions and bibliography database(s)
%\bibliography{IEEEabrv,../bib/paper}
%
% <OR> manually copy in the resultant .bbl file
% set second argument of \begin to the number of references
% (used to reserve space for the reference number labels box)
%\begin{thebibliography}{1}

%\bibitem{}
%Hattori, L.P.; Lanza, M., \emph{"On the nature of commits"} in Automated Software Engineering - Workshops, 2008. ASE Workshops 2008. 23rd IEEE/ACM International Conference on , vol., no., pp.63-71, 15-16 Sept. 2008
%\bibitem{}
%Livshits,B.; Zimmerman,T.; \emph{"DynaMine: Finding Common Error Patterns by Mining Software Revision Histories"} in Proceedings of the 10th European software engineering conference held jointly with 13th ACM SIGSOFT international symposium on Foundations of software engineering, pp.296-305
%\bibitem{}
%Mockus, A.; Votta, L.G., \emph{"Identifying reasons for software changes using historic databases"} in Software Maintenance, 2000. Proceedings. International Conference on , vol., no., pp.120-130, 2000
%\bibitem{}
%Ratzinger,J.; Sigmund,T.; Gall,H.; \emph{"On the Relation of Refactoring and Software Defects"} in MSR '08, Proceedings of the 2008 international working conference on Mining software repositories, pp. 35-38
%\bibitem{}
%Hindle,A.; German,D.; Holt,R.; \emph{"What do large commits tell us?: a taxonomical study of large commits"} in MSR '08, Proceedings of the 2008 international working conference on Mining software repositories, pp.99-108
%\bibitem{}
%Oliva,G.; Steinmacher,I.; Wiese,I.; Aurelio,M.; \emph{"What Can Commit Metadata Tell Us About Design Degradation?"} in IWPSE 2013, Proceedings of the 2013 International Workshop on Principles of Software Evolution, pp.18-27
%\bibitem{}
%Phillips,S.; Sillito,J.; Walker,R.; \emph{"Branching and merging: an investigation into current version control practices"} in CHASE '11 Proceedings of the 4th International Workshop on Cooperative and Human Aspects of Software Engineering, pp.9-15
%\bibitem{}
%Agrawal, K.; Amreen, S.; Mockus, A., \emph{"Commit Quality in Five High Performance Computing Projects"} in Software Engineering for High Performance Computing in Science (SE4HPCS), 2015 IEEE/ACM 1st International Workshop on , vol., no., pp.24-29, 18-18 May 2015
%\bibitem{}
%Steff, M.; Russo, B., \emph{"Co-evolution of logical couplings and commits for defect estimation"} in Mining Software Repositories (MSR), 2012 9th IEEE Working Conference on , vol., no., pp.213-216, 2-3 June 2012
%\bibitem{}
%Williams,C.; Spacco,J.; \emph{"Branching and Merging in the Repository"} in MSR '08, Proceedings of the 2008 international working conference on Mining software repositories, pp.19-22
%\bibitem{}
%Kolassa,C.; Riehle,D.; Salim,M.; \emph{"The Empirical Commit Frequency Distribution of Open Source Projects"} in WikiSym '13, Proceedings of the 9th International Symposium on Open Collaboration, Article No.18
%\bibitem{}
%Ray,B.; Posnett,D.; Filkov,V.; Devanbu,P;  \emph{A Large Scale Study of Programming Languages and Code Quality"} in Github;  FSE 2014, Proceedings of the 22nd ACM SIGSOFT International Symposium on Foundations of Software Engineering, pp.155-165
%\bibitem{}
%Phipps,G; \emph{"Comparing Observed Bug and Productivity Rates for Java and C++" } in Software—Practice \& Experience; vol.29, issue 4, pp.345-358, April 10, 1999 
%\bibitem{}
%Prechelt, L., \emph{"An empirical comparison of seven programming languages"} in Computer , vol.33, no.10, pp.23-29, Oct 2000
%\bibitem{}
%Fourment,M.; Gillings,M.; \emph{"A comparison of common programming languages used in bioinformatics"} in BMC Bioinformatics Feb 2008
%\bibitem{}
%Furia,C.; Nanz,S.; \emph{"A Comparative Study of Programming Languages in Rosetta Code"} in ICSE '15, Proceedings of the 37th International Conference on Software Engineering, vol.1, pp.778-788
%\bibitem{}
%Iowa State University of Science and Technology,  \emph{"Boa - Mining Ultra-Large-Scale Software Repositories"}, http://boa.cs.iastate.edu/


%\end{thebibliography}

% biography section
% 
% If you have an EPS/PDF photo (graphicx package needed) extra braces are
% needed around the contents of the optional argument to biography to prevent
% the LaTeX parser from getting confused when it sees the complicated
% \includegraphics command within an optional argument. (You could create
% your own custom macro containing the \includegraphics command to make things
% simpler here.)
%\begin{biography}[{\includegraphics[width=1in,height=1.25in,clip,keepaspectratio]{mshell}}]{Michael Shell}
% or if you just want to reserve a space for a photo:

% You can push biographies down or up by placing
% a \vfill before or after them. The appropriate
% use of \vfill depends on what kind of text is
% on the last page and whether or not the columns
% are being equalized.

%\vfill

% Can be used to pull up biographies so that the bottom of the last one
% is flush with the other column.
%\enlargethispage{-5in}




% that's all folks
\end{document}


